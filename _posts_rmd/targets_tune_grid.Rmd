---
title: "Targets and tidymodels"
author: "Nathan Layman"
date: "`r Sys.Date()`"
output:
  md_document:
    variant: gfm
knit: >
  (function(input_file, encoding) {
    metadata <- rmarkdown::yaml_front_matter(input_file)
    output_file = paste0("_posts/", Sys.Date(), "-", gsub(" ", "-", metadata$title), ".md")
    rmarkdown::render(input = input_file, output_file = xfun::from_root(output_file))
  })
---

```{=html}
---
layout: post
title: Tidymodels hyperparameter tuning with targets dynamic branching
subtitle: Dynamic branching with tune_grid()
thumbnail-img:  /assets/img/tidy_and_targets.png    
tags: [data, R, tricks]
mathjax: true
---
```
```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.path=paste0("../assets/img/", Sys.Date(), "-", gsub(" ", "-", metadata$title), "_files/"))

library(tidyverse)
library(openai)
library(jsonlite)
library(stringdist)
library(DT)
library(here)
library(rmarkdown)
library(ggdendro)

old_posts <- list.files(here::here("_posts"))
old_posts <- grep(gsub(" ", "-", metadata$title), old_posts, value = T)
file.remove(here::here("_posts", old_posts))
```

## Machine learning model parameters

At it's core machine learning is a collection of statistical and mathematical techniques to analyze and draw inferences from patterns in data. The way machine learning models 'learn' is by adjusting sets of parameters in order to minimize error and maximize predictive performance. For example, in linear regression, the parameters are the regression coefficients **$$\beta$$** and the intercept. For neural networks, the parameters are the weights and biases of each neron. In tree based models, the parameters include the collection of tree topologies and their associated leaf weights.

## Parameters vs hyper-parameters

While model parameters are learned from the data during training, hyper-parameters control *how* models learn from the data. For example for [xgboost](https://xgboost.readthedocs.io/en/stable/tutorials/model.html) hyperparameters include *$$\gamma$$* a regularization parameter that controls the cost of tree complexity and the *$$max\_depth$$* which limits how deep each tree can grow.

Hyper-parameters must be specified prior to model fitting, but how to chose the best ones? This is where cross-validation comes in. By splitting the data into training, validation, and testing datasets it's possible to tune the hyper-parameters on a subset of the data and then fit and evaluate the final model performance against a hold-out data set.

## Parallel hyper-parameter tuning

Hyper-parameter tuning is an important step in machine learning pipelines. It generally involves fitting a lot of models across a grid of different hyper-parameters in order to find the optimal set. Unfortunately fitting so many models takes a lot computational resources and time. Fortunately the problem is what is commonly known as [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel). That means each of the models can be fit independently and in parallel.

## Tidymodels tune_grid()

In the [tidymodels](https://www.tidymodels.org/) package in R, hyper-parameter tuning is handled by the **`tune_grid()`** function. The **`tune_grid()`** function can handle parallelism using the [future](https://future.futureverse.org/) package in R. However there are some limitations, for example there isn't an easy way to monitor progress. Another problem is that if one of the models stochastically fails it can disrupt the entire tuning process. Lastly it can be useful to control whether parallelization occurs, over the cross-validation folds, over the list of hyper-parameter sets, or a cross of both.

## Targets dynamic branching

Fitting a lot of models involves orchestrating many different processes - something that pipeline management tools like [targets](https://books.ropensci.org/targets/) are specifically designed to do. In targets, fitting a lot of models at once can be accomplished using [dynamic branching](https://books.ropensci.org/targets/dynamic.html). Dynamic branching sets up a subprocess for each model. Targets can track the progress of branches as they complete using **`tar_watch()`** and **`tar_poll()`** and any failed branches can be re-run after tuning completes by setting `error = "null"` within the dynamically branched target. The `pattern` argument in the target can also be used to specify how branching occurs - mapping over the cross validation folds, the hyper-parameters, or using `cross()` to fit a model to every combination.

## targets and tidymodels

Below is an example targets based tidymodel pipeline. It uses a a custom function, **`tune_grid_branch()`** within a dynamically branched target to tune the hyper-parameters of a Bayesian Additive Regression Trees (BART) model. The key target is `bart_tuned` which is set to branch over every combination of cross-validation fold and hyper-parameter set combination by `pattern = cross(training_data_folds, bart_gridsearch)`.

```
  tar_target(analysis_recipe, recipe(paste(response_variable, "~ .") |>
as.formula(), data = analysis_data_train) |>
               step_naomit() |>
               step_string2factor(all_string()) |>
               step_novel(all_nominal(), -all_outcomes()) |>
               step_dummy(all_nominal(), -all_outcomes()) |>
               step_zv(all_predictors())),
               
# Set up the BART model
  tar_target(bart_model, 
             parsnip::bart(trees = tune(),
                           prior_terminal_node_coef = tune(),
                           prior_terminal_node_expo = tune()) |> 
               set_engine("dbarts") |>
               set_mode("classification")),
  
  # Set up the BART model workflow
  tar_target(bart_workflow, workflow() |> 
               add_recipe(analysis_recipe) |> 
               add_model(bart_model)),
  
  # Set up the hyper-parameter grid search.
  # Automatically extract the parameters to tune across.
  tar_target(bart_gridsearch, bart_workflow |> 
               extract_parameter_set_dials() |>
               dials::grid_latin_hypercube(size = 10)),
  
  # Tune the model
  tar_target(bart_tuned, tune_grid_branch(workflow = bart_workflow,
                                           gridsearch_params = bart_gridsearch,
                                           training_data_folds = training_data_folds),
             pattern = cross(training_data_folds, bart_gridsearch))
             
  # Extract the best set of hyper-parameters
  tar_target(bart_best_params, select_best_params(bart_tuned, metric = "roc_auc")),
  
```

## tune_grid_branch

Below is my implementation of a dynamic branch friendly tune_grid function. In addition to the benefits described above it also tracks both fit time and the amount of memory required for each model fit. Using targets it's possible to just fit a few of the models in order to profile your model tuning workflow. This can be accomplished by setting `pattern = head(cross(training_data_folds, bart_gridsearch), 5)` which will fit the first 5 models only. You can then get a sense of how long the fitting will take and how much resources each branch will require. The best part is that when you're ready to do the full run you can remove the `head()` part of the command and targets _won't need to re-run those first 5 models_. I'm certain there's room to improve this but at least to me it seems like a very powerful combination.

```
#' A function to manually tune a tidymodels workflow
#'
#' @param workflow # A tidymodels workflow with recipe and model already attached
#' @param gridsearch_params # A tibble where each row is a set of hyperparameters
#' @param training_data_folds # A tibble where each row is a training data fold
#' @param metrics # A set of metrics produced by yardstick::metric_set()
#'
#' @return Returns a tibble with fit performance metrics, fit time, and the ram used while fitting
#' @export
#'
#' @examples
tune_grid_branch <- function(workflow, 
                             gridsearch_params, 
                             training_data_folds, 
                             metrics = metric_set(pr_auc,           # Precision-Recall AUC
                                                  roc_auc,          # ROC AUC
                                                  accuracy,         # Accuracy
                                                  f_meas,           # F1 Score
                                                  recall,           # Recall
                                                  precision),
                             verbose = F) {
  
  # 2D:
  # Add in dynamic response variable and fix the problem with as.factor
  performance <- map_dfr(1:nrow(training_data_folds), function(i) {
    map_dfr(1:nrow(gridsearch_params), function(j) {
      
      rsamp <- rsample::manual_rset(training_data_folds[i,]$splits, training_data_folds[i,]$id)
      params <- gridsearch_params[j,]
      if(verbose) print(rsamp |> bind_cols(params))
      
      # Grab start time
      start_time <- Sys.time()
      
      # Fit the model against the training data
      mem_usage_bytes <- profmem::profmem({
        fold_param <- tune::tune_grid(workflow,
                                      resamples = rsamp, # This looks promising...
                                      grid = params,
                                      metrics = metrics)
      })
      
      # Report fit performance metrics
      fold_param |> mutate(id = branch = targets::tar_name(),
                           mem_usage_bytes = sum(mem_usage_bytes$bytes, na.rm=T),
                           fit_time = start_time - Sys.time())
    })
  })
  
  # Clean up environment in case targets tries to store extra stuff
  rm(list=setdiff(ls(), "performance"))
  
  # Return performance
  performance
}


```
